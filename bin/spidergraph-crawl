#!/usr/bin/env babel-node --

import { graphql } from 'graphql'
import loadQueryStore from "../src/queryStore.js";
import urlLoader from "../src/browser";
import schema from "../src/schema";
import program from 'commander';
import { transports } from '../src/logger';

import { Observable } from "rxjs";
import { take, concatMap, expand, mergeMap } from "rxjs/operators";


const fetch = async (urls, queryStore, browser) => {
  const results = await Promise.all(urls.map(async url => {
    return await Promise.all(queryStore.map(async (query) => {
      const pattern = new RegExp(query.config.match.url);
      if (!pattern.test(url)) return;
      const page = await browser.load(url).then(page => page);

      const result = await graphql(schema, query.queryString, {}, {page: page}).then((result) => {
        return result
      })

      page.close();

      return {
        info: {
          query: query.queryName,
          url: url
        },
        data: result.data
      }
    }))
  }));
  return [].concat(...results).filter(n => n)
}

const scrape = async (urls) => {
  transports.console.level = 'error'
  const queryStore = await loadQueryStore(schema);
  const browser = await urlLoader
  const listCrawler = Observable.fromPromise(fetch(urls, queryStore, browser))
    .pipe(
      expand(event => {
        const next = event[0].data.crawler.pagination.next
        return Observable.fromPromise(fetch([next], queryStore, browser))
      }
    ),
    take(2),
    concatMap(event => event[0].data.crawler.collection)
  )

  const throttled = listCrawler.pipe(
    mergeMap(event => Observable.fromPromise(fetch([event.url], queryStore, browser)), null, 1)
  );

  throttled.subscribe(link => console.log(JSON.stringify(link)));
}

const list = (val) => {
  return val.split(",");
}

program
  .arguments('<urls>')
  .action(async (urls) => (await scrape(list(urls))))
  .parse(process.argv);
